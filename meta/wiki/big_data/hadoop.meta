a:2:{s:7:"current";a:8:{s:4:"date";a:2:{s:7:"created";i:1435507982;s:8:"modified";i:1435507982;}s:7:"creator";s:13:"Alina GHERMAN";s:4:"user";s:4:"veve";s:11:"last_change";a:7:{s:4:"date";i:1435507982;s:2:"ip";s:14:"77.242.202.244";s:4:"type";s:1:"C";s:2:"id";s:20:"wiki:big_data:hadoop";s:4:"user";s:4:"veve";s:3:"sum";s:7:"created";s:5:"extra";s:0:"";}s:5:"title";s:6:"Hadoop";s:11:"description";a:2:{s:15:"tableofcontents";a:1:{i:0;a:4:{s:3:"hid";s:6:"hadoop";s:5:"title";s:6:"Hadoop";s:4:"type";s:2:"ul";s:5:"level";i:1;}}s:8:"abstract";s:301:"Hadoop

Is a framework in response to Big Data problems.

	*  analysing large volumes of data
	*  processing large volumes of data
	*  big computers cost a lot (monolitic computing) ==> use a lot of smaller computers (distributed computing)
	*  data access is one of the bottlenecks of processing data";}s:8:"internal";a:2:{s:5:"cache";b:1;s:3:"toc";b:1;}s:8:"relation";a:1:{s:10:"firstimage";s:0:"";}}s:10:"persistent";a:4:{s:4:"date";a:1:{s:7:"created";i:1435507982;}s:7:"creator";s:13:"Alina GHERMAN";s:4:"user";s:4:"veve";s:11:"last_change";a:7:{s:4:"date";i:1435507982;s:2:"ip";s:14:"77.242.202.244";s:4:"type";s:1:"C";s:2:"id";s:20:"wiki:big_data:hadoop";s:4:"user";s:4:"veve";s:3:"sum";s:7:"created";s:5:"extra";s:0:"";}}}
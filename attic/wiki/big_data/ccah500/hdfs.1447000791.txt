====== HDFS (17%) ======

===== Describe the function of HDFS Daemons ===== 
  * **NameNode:** manages the filesystem metadadata and persists all file changes metadata into 2 files:
        * **edits:**  changes made to the metadatasystem since the last fsimage was created (since the last checkpoint). There is one edit file per transaction (created)
        * **fsimage:** an image of the metadata system

The dataNode sends lists of blocks to the Namenode in order to inform him what blocks does it have. The namenode will save this information in memory.

Clients are connecting to the NN in order to perform filesystem operations. 

The Namenode filesystem puts up all metadata information in memory in order to be able to do fast lookup.

  * **Secondary NameNode:** makes a merge of the edits file and the fsimage file for the NameNode.
  *** DataNode:** stores and retrieves blocks of data.It has to send hartbeats to the Namenode ( list of blocks).
<code>
# format the new filesystem 
# create the first fsimage, edits 
# to be run on the namenode
bin/hadoop namenode -format

# start all hdfs daemons 
# to be run on the namenode
bin/start-dfs.sh
</code>


=====  Describe the normal operation of an Apache Hadoop cluster, both in data storage and in data processing. ===== 

  * Data storage is assured by the HDFS service wich is a distributed filesystem. 
  * Data processing is historicaly done by MapRecuce, now by YARN. 

Each time you have a script in Hive/ Pig, it is transformed in MapReduce code.

TODO

==== Data Processing ====
sources: 
  * http://blog.cloudera.com/blog/2015/06/architectural-patterns-for-near-real-time-data-processing-with-apache-hadoop/


In Map Reduce Jobs:
  * Job Resources and metadata are shared in HDFS for the Application Master to access
  * User code runs on the Node Managers




===== Identify current features of computing systems that motivate a system like Apache Hadoop. =====
  * big computers cost a lot (monolitic computing) ⇒ use a lot of smaller computers (distributed computing)
  * data access is one of the bottlenecks of processing data on a big computer ⇒ since here we  have  it on multiple computers (replication), we can read faster large blocks of sequential data
  * handle failure ⇒ on a big computer a failure is a failure , here we can handle several workers failure, and a namenode failure


===== Classify major goals of HDFS Design =====
  * replicates data accross multiple nodes. The replication factor is defined by a configuraiton. This will help       
      * The processing speed and the read speed. 
      * Fault tolerance
  * the block size is significantly larger than the block size of ext3 or any other classical filesystem. By default is 64 MB and there are some that increase it to 256 MB, or event 1GB. This will :
        * The processing speed and the read speed.
        * Increase the occupied size on your disk if you have small files ==> see small files problem
  * Master-slave architecture
  * 2 security levels
  * “Moving Computation is Cheaper than Moving Data”
  * write-once-read-many access model
  * Large Data Sets Not tens of million of small files



===== Given a scenario, identify appropriate use case for HDFS Federation =====

**What is it?**
The fact that there is the SNN in order to take some tasks from the NN. 
Is different than having multipe clusters because each DN can have blocks from different NN (each DN has a block pool for each Namespace).

http://fr.slideshare.net/ydn/hug-march-hdfs-federation 

**Why shoud I use it?**
  * It works around the memory limitation of the NN.
  * Allow us to use namespace partitioning to control the availability of different slices of the filesystem.

**Important Note:**
  * HA and Federation are ortogonal feature. We can enable them independently.

==== Scenario 1: Performance ====
The Namenode process the entire metadata into memory. So there is a limitation between the number of objects that we can store and the namenode memory. 
  * the startup can be slowed down if there are a lot of items to put up in memory
  * debugging with a single larger JVM is harder than with 2 smaller JVMs

==== Scenario 2: Application isolation====
When we have a single namenode the experimental applications can affect the production ones if we work on the same cluster.

==== Scenario 3: Failure ====
If the Namenode Federation is not used the failure of the namenode will bring down the entire cluster.

==== Scenario 4: Big Big Cluster====
A namenode with 50GB Of memory and 200 million objects can support 400 DataNodes, 12 PB of storage. So in order to support ultiple Datanodes, we could get a bigger machine or use the HDFS Federation.


===== Identify components and daemon of an HDFS HA-Quorum cluster =====
**WHY use HDFS HA? **
  * The long time recovery if the NN fails
        * start a new primary NN with one of the filesystem metadata replicas 
              * The NN has to load the namespace image into memory
              * The NN has to replay its edit logs
              * The NN has to receive enough block reports from the DN to leave safe mode
        * configure DN and clients to use this new NN
  * The NN is a SPOF if HDFS HA is not used

**Daemons **
  * 2 NN: at any point one NN is in active state and the other is in standby state. The active NN is responding to all client requests and the standby NN is maintaining enough state to provide fast fail over. The two NN are reading the edit logs from a high available shared storage the QJM (Quorum Journal Manager)
  * At least 3 JN: Journal Node -> We have at least 3 in order to have a majority of JN. The 2 NN communicate to the JN . The group of JN is called QJM (Quorum Journal Manager)
  * SN : in the HA cluster the SN is still doing the checkpoints, __but there is no need__. This is kept in order to be able to change the configuration from a non HA cluster to a HA cluster without interruption. However, you can reuse the machine of the Secondary Namenode for the Standby Namenode.


**Configurations**
  * Manual or automatic failover. In the automatic failover there is an additional process: failover controller that monitors the health of the process and coordinates state transitions.
  * The clients must be configured to handle NN failover
  *

===== Analyze the role of HDFS security  =====
**source:** Hadoop the definitive Guide by Tom White


  * (authorization mechanism) File permissions system in HDFS prevents unauthorized users to delete data
  * Kerberos: "says that the user us who says she is". Steps done by Kerberos:
                 * **Authentification (not user-level action):** The client asks for a timestamped ticket (TGT: Ticket-Granting Ticket) to the Kerberos Authentification Server. The TGT normally last for 10 hours.
                 * **Authorisation (not user-level action):** The clients uses the received TGT in order to ask for a service to the Ticket-Granting Server.
                 * **Service request** (user-level action demanding a password: kinit command): The client uses the service ticket to authentificate itself to the server(namenode, resource manager) that is providing the service.

**Note:**
If you don't want to be prompted for a password ==> creat a keytab file (ktutil command)

**Enable Kerberos steps:**
  * install kerberos
  * run a KDC (hadoop does not come with one)
  * hadoop.security.authentification ==> kerberos
  * hadoop.security.author ==> true
  * dfs.block.access.tocke.enable


**Delegation Tokens**: Hadoop uses delegation in order not to over load the KDC with unnecessary tocken requests. The delegation tocken is generated by the server (namenode) and is used as a shared secred between the client and the server. On the first call the client has no tocken so it uses Kerberos to authentificate. After the authentification succedes, the client gets a delegation tocken from the namenode.
HDFS uses a special delegation tocken : the block access tocken that is also generated by the namenode.


TODO
===== Determine the best data serialization choice for a given scenario =====

TODO find an example
===== Describe file read and write paths =====
==== Read ====
  * source image: http://www.guru99.com/learn-hdfs-a-beginners-guide.html
  * source steps: "Hadoop Operations" By Eric Sammer
{{:wiki:big_data:readhdfs.png|}}

  * We assume that a file X is on the filesystem
  * The client asking for the file X has to have:
          * the hadoop client library (JARs)
          * a copy of the cluster configuration that specifies the NN location
  * The client ask the NN the permission to read the file X
  * The NN is checking the validity of the client (if kerberos) and the permissions of the file X.
  * If everything is OK, the NN sends to the client the first Block ID of the file X and the list of DataNodes on wich he can find it (sorted by distance to client)
  * The client can contact the most appropiate DN in order to read the block data.
  * This process repeats until all blocks in the file have been read. 


==== Write ====
source image: http://www.guru99.com/learn-hdfs-a-beginners-guide.html
{{:wiki:big_data:writehdfs.png|}}
===== Identify the commands to manipulate files in the Hadoop File System Shell =====
TODO
====== 4. Hadoop Cluster Installation and Administration (25%) ======

=====Handle disk and machine failures =====
  * HDFS replicates data blocks on nodes on different racks

**If a NodeManager fails: ** 
    * the Resource Manager removes it from the list of the active nodes
    * the tasks on this node are considered as failed by the ApplicationManager
    * if the ApplicationManager fails => failed application


**If the ResourceManager fails: ** 
  * No application or task can be launched
  * can be configured in HA


**If NameNode goes down:**
  * if we are not in HA , the cluster is inaccessible

**If DataNode goes down:**
  * the NameNode will re-replicate the blocks that were on this machine
  * the jobs will access the data on the remaining machines
===== Logging configuration =====
==== Daemon Log ====
  * each hadoop daemon will produce two log files: .log and .out (stdout+stderr)
  *  /var/log/hadoop-<component> (configurable location in hadoop-env.sh)
  * usually maximum disk space for logs: 5 GB
  * the logs retention is configurable:
        * the RFS: RollingFileAppender
        * /etc/hadoop/conf/log4j.properties
  * exemple:
      * /var/log/hadoop-hdfs/hadoop-hdfs-machinename.log
      * /var/log/hadoop-yarn/yarn-yarn-machinename.log
  * change the daemonlog level: 

        - shell: <code>hadoop daemonlog â€“setlevel <daemon_host>:<daemon_HTTP_port> <logger> <level-like ERROR, WARN, INFO..> </code>(CHANGE NOT PERSISTENt AFTER RESTART)
        - web: <code>http://<daemon_host>:<daemon_HTTP_port>/logLevel </code>(CHANGE NOT PERSISTENt AFTER RESTART)
       - PERSISTENT CHANGE: /etc/hadoop/conf/log4j.properties


==== Job Log ====
  * The application Manager Log
  * Task stdout, stderr, syslog output
  * Counters


  * The job logs can be aggregated on HDFS
  * Retention: <code>yarn.log-aggregation.retain-seconds</code>
  * If you want to access the logs from hte command line you need the job application ID
        * <code> yarn application -list -appStates FINISHED | grep 'word count'</code>
        * <code> yarn logs -applicationId application_1392918622651_0004 </code>
=====   Hadoop metrics and cluster health monitoring =====
  * Cloudera Manager can e used for cluster monitoring
  * Monitor the Hadoop daemons:
      * Alert an operator if a daemon goes down
      * service hadoop-component-daemon_name status
  * Monitor disks and disk partitions
      * Alert imediatly if a disk goes down
      * Send a warning if disk > 80%
  * Montor CPU usage on master nodes
  * Monitor swap on all nodes
  * Monitor network transfer speeds
  * Monitor HDFS health
      * HA configuration: check the size of edit logs
      * non HA: check the age of fsimage and the size of edit files
  * Monitor the log sizes (some developpers will create large task logs)
      * logs file aggregation ==> logs are moved to HDFS
  * Monitor local disk


===== Identify the function and purpose of available tools for cluster monitoring =====
  * Cloudera Manager: disk, CPU, node failure..
  * Other monitoring tool: configure to access the JMX ports of HAdoop or the HAdoop metrics sink
  * Hadoop Metrics2 Framework: all CDH daemons, reads metrics sink configuration from
        * /etc/hadoop/conf/hadoop-metrics2.properties
  * We can configure HAdoop:
      * to publish metrics using the Hadoop Metrics2 framework: /etc/hadoop/conf/hadoop-metrics2.properties
      * to broadcast the HMX metrics: /etc/hadoop/conf/hadoop-env.sh
===== Install all the ecoystme components in CDH 5 =====
==== Hadoop ====
<code>
#configure the hosts 
/etc/hosts

#check name 
uname -n

#check name 
echo $HOSTNAME

#connect to one machine and: 
sudo yum install --assumeyes hadoop-conf-pseudo

#format the NN (namenode metadata: /var/lib/hadoop-hdfs/cache/hdfs/dfs/name.)
sudo -u hdfs hdfs namenode -format 

#start hdfs servicess
sudo service hadoop-hdfs-namenode start
sudo service hadoop-hdfs-secondarynamenode start
sudo service hadoop-hdfs-datanode start

#create needed directories
sudo -u hdfs hadoop fs -mkdir /tmp
sudo -u hdfs hadoop fs -mkdir -p /tmp/hadoop-yarn/staging/history/done_intermediate
sudo -u hdfs hadoop fs -chown -R mapred:mapred /tmp/hadoop-yarn/staging
sudo -u hdfs hadoop fs -chmod -R 1777 /tmp
sudo -u hdfs hadoop fs -mkdir -p /var/log/hadoop-yarn
sudo -u hdfs hadoop fs -chown yarn:mapred /var/log/hadoop-yarn

#start yarn services
sudo service hadoop-yarn-resourcemanager start
sudo service hadoop-yarn-nodemanager start
sudo service hadoop-mapreduce-historyserver start

#check that they are running
sudo jps

#user directory
sudo -u hdfs hadoop fs -mkdir -p /user/veve
sudo -u hdfs hadoop fs -chown training /user/veve

#create the weblog directory
hadoop fs -mkdir weblog
</code>
==== Impala ====
==== Flume====
==== Oozie====
==== Hue====
==== Cloudera Manager====
==== Sqoop====
==== Hive====
==== Pig ====
===== Identify the function and purpose of available tools for managing the Apache Hadoop file system =====